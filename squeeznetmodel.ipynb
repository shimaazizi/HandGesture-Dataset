{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1GBac-lsjSROuK6MRYWEK2tYek93fszAp","authorship_tag":"ABX9TyPXPrF4tHwoO0qqUylI8t5r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":13,"metadata":{"id":"ceYpqx_r-ciA","executionInfo":{"status":"ok","timestamp":1719665337537,"user_tz":-210,"elapsed":423,"user":{"displayName":"shima azizi","userId":"03537820371144014436"}}},"outputs":[],"source":["import os\n","import math\n","import random\n","from PIL import Image\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","from torchvision.models import SqueezeNet1_0_Weights\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","from os.path import join"]},{"cell_type":"code","source":["Dataset = \"/content/drive/MyDrive/Dataset\""],"metadata":{"id":"Nn-Q8FyC_uyi","executionInfo":{"status":"ok","timestamp":1719664603289,"user_tz":-210,"elapsed":9,"user":{"displayName":"shima azizi","userId":"03537820371144014436"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class CustomDataGenerator(Dataset):\n","    def __init__(self, x, y, label_encoder, batch_size=32, target_size=(128, 128),\n","                 rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n","                 zoom_range=0.2, horizontal_flip=True, train=True):\n","        self.x = []\n","        self.y = []\n","        self.label_encoder = label_encoder\n","        self.batch_size = batch_size\n","        self.target_size = target_size\n","        self.rotation_range = rotation_range\n","        self.width_shift_range = width_shift_range\n","        self.height_shift_range = height_shift_range\n","        self.zoom_range = zoom_range\n","        self.horizontal_flip = horizontal_flip\n","        self.train = train\n","        self.skipped_images = 0\n","\n","        # Filter out broken images during initialization\n","        for img_path, label in zip(x, y):\n","            if os.path.exists(img_path):\n","                try:\n","                    with Image.open(img_path) as img:\n","                        img.verify()  # Verify the image\n","                    self.x.append(img_path)\n","                    self.y.append(label)\n","                except Exception as e:\n","                    print(f\"Skipping corrupted image {img_path}: {e}\")\n","                    self.skipped_images += 1\n","            else:\n","                print(f\"Image path does not exist: {img_path}\")\n","\n","        self.classes = set(item.split(\"_\")[0] for item in os.listdir(os.path.dirname(self.x[0])))\n","\n","        # Define data augmentation transforms\n","        self.transforms = transforms.Compose([\n","            transforms.Resize(self.target_size),\n","            transforms.RandomHorizontalFlip() if self.horizontal_flip else transforms.Lambda(lambda x: x),\n","            transforms.RandomRotation(self.rotation_range) if self.rotation_range else transforms.Lambda(lambda x: x),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, index):\n","        image_path = self.x[index]\n","        label = self.y[index]\n","\n","        image = Image.open(image_path).convert('RGB')\n","        image = self.transforms(image)\n","\n","        label_encoded = self.label_encoder.transform([label])[0]\n","        return image, label_encoded\n","\n","\n"],"metadata":{"id":"vB88_cxM_rLq","executionInfo":{"status":"ok","timestamp":1719665090762,"user_tz":-210,"elapsed":459,"user":{"displayName":"shima azizi","userId":"03537820371144014436"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def freeze_layers(model):\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","\n","\n","#weights = \"/home/shima/.cache/torch/hub/checkpoints\"\n","def create_squeezenet_model(num_classes=4, freeze=True):\n","    model = models.squeezenet1_0(pretrained=True)\n","    if freeze:\n","        freeze_layers(model)\n","\n","\n","    model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1))\n","    model.num_classes = num_classes\n","\n","    # Initialize the new classifier layer\n","    nn.init.normal_(model.classifier[1].weight, mean=0.0, std=0.01)\n","    if model.classifier[1].bias is not None:\n","        nn.init.constant_(model.classifier[1].bias, 0)\n","\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","    return model, optimizer, criterion\n","\n","def dataset_split(dataset_path: str, test_split: float = 0.2, val_split: float = 0.2):\n","    images_files = []\n","    labels = []\n","\n","    for item in os.listdir(dataset_path):\n","        item_path = join(dataset_path, item)\n","        if os.path.isdir(item_path):\n","            class_label = item\n","            for image_file in os.listdir(item_path):\n","                image_path = join(item_path, image_file)\n","                images_files.append(image_path)\n","                labels.append(class_label)\n","\n","    print(f\"Total number of images: {len(images_files)}\")\n","    print(f\"Unique class labels: {set(labels)}\")\n","\n","    x_train, x_test, y_train, y_test = train_test_split(images_files, labels, test_size=test_split, random_state=42, stratify=labels)\n","    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_split, random_state=42, stratify=y_train)\n","\n","    print(f\"Train samples: {len(x_train)}\")\n","    print(f\"Validation samples: {len(x_val)}\")\n","    print(f\"Test samples: {len(x_test)}\")\n","\n","    return x_train, y_train, x_val, y_val, x_test, y_test\n","\n","if __name__ == '__main__':\n","    dataset_path = \"/content/drive/MyDrive/Dataset\"\n","    x_train, y_train, x_val, y_val, x_test, y_test = dataset_split(dataset_path)\n","    print(\"Original data:\")\n","    print(\"train:\", Counter(y_train))\n","    print(\"val:\", Counter(y_val))\n","    print(\"test:\", Counter(y_test))\n","\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(y_train + y_val + y_test)\n","\n","    train_dataset = CustomDataGenerator(x_train, y_train, label_encoder, train=True, batch_size=16, target_size=(128, 128))\n","    print(f\"Skipped images in train dataset: {train_dataset.skipped_images}\")\n","\n","    val_dataset = CustomDataGenerator(x_val, y_val, label_encoder, train=False, batch_size=32, target_size=(128, 128))\n","    print(f\"Skipped images in validation dataset: {val_dataset.skipped_images}\")\n","\n","    test_dataset = CustomDataGenerator(x_test, y_test, label_encoder, train=False, batch_size=32, target_size=(128, 128))\n","    print(f\"Skipped images in test dataset: {test_dataset.skipped_images}\")\n","\n","\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32)\n","    test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","    num_classes = len(label_encoder.classes_)\n","    model, optimizer, criterion = create_squeezenet_model(num_classes, freeze=True)\n","\n","    num_epochs = 50\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        train_correct = 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            train_correct += (predicted == labels).sum().item()\n","\n","        train_loss = train_loss / len(train_loader.dataset)\n","        train_accuracy = train_correct / len(train_loader.dataset)\n","\n","        model.eval()\n","        val_loss = 0.0\n","        val_correct = 0\n","\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                outputs = model(inputs)\n","\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * inputs.size(0)\n","                _, predicted = torch.max(outputs, 1)\n","                val_correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(val_loader.dataset)\n","        val_accuracy = val_correct / len(val_loader.dataset)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, '\n","              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n","              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n","\n","    model.eval()\n","    test_correct = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            test_correct += (predicted == labels).sum().item()\n","\n","    test_accuracy = test_correct / len(test_loader.dataset)\n","    print(f'Test Accuracy: {test_accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fH88wHtgBkjz","executionInfo":{"status":"ok","timestamp":1719669365522,"user_tz":-210,"elapsed":1570872,"user":{"displayName":"shima azizi","userId":"03537820371144014436"}},"outputId":"6ca9ec4a-a17f-450e-e69d-c5d2866fad78"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of images: 200\n","Unique class labels: {'Fist', 'ThumbsUp', 'PeaceSign', 'OpenPalm'}\n","Train samples: 128\n","Validation samples: 32\n","Test samples: 40\n","Original data:\n","train: Counter({'OpenPalm': 32, 'Fist': 32, 'PeaceSign': 32, 'ThumbsUp': 32})\n","val: Counter({'ThumbsUp': 8, 'PeaceSign': 8, 'OpenPalm': 8, 'Fist': 8})\n","test: Counter({'Fist': 10, 'PeaceSign': 10, 'OpenPalm': 10, 'ThumbsUp': 10})\n","Skipped images in train dataset: 0\n","Skipped images in validation dataset: 0\n","Skipped images in test dataset: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Train Loss: 1.4375, Train Acc: 0.2969, Val Loss: 1.2250, Val Acc: 0.5625\n","Epoch 2/50, Train Loss: 1.2125, Train Acc: 0.4531, Val Loss: 1.0055, Val Acc: 0.6875\n","Epoch 3/50, Train Loss: 0.9990, Train Acc: 0.6328, Val Loss: 0.9225, Val Acc: 0.7188\n","Epoch 4/50, Train Loss: 0.7845, Train Acc: 0.7578, Val Loss: 0.7997, Val Acc: 0.7188\n","Epoch 5/50, Train Loss: 0.7346, Train Acc: 0.7656, Val Loss: 0.8374, Val Acc: 0.6875\n","Epoch 6/50, Train Loss: 0.6378, Train Acc: 0.7500, Val Loss: 0.7110, Val Acc: 0.8125\n","Epoch 7/50, Train Loss: 0.5244, Train Acc: 0.8438, Val Loss: 0.7233, Val Acc: 0.6875\n","Epoch 8/50, Train Loss: 0.5025, Train Acc: 0.8594, Val Loss: 0.6519, Val Acc: 0.7500\n","Epoch 9/50, Train Loss: 0.4473, Train Acc: 0.9141, Val Loss: 0.7152, Val Acc: 0.8125\n","Epoch 10/50, Train Loss: 0.4595, Train Acc: 0.8594, Val Loss: 0.7307, Val Acc: 0.7188\n","Epoch 11/50, Train Loss: 0.4387, Train Acc: 0.8750, Val Loss: 0.6881, Val Acc: 0.7188\n","Epoch 12/50, Train Loss: 0.3866, Train Acc: 0.9141, Val Loss: 0.5664, Val Acc: 0.8125\n","Epoch 13/50, Train Loss: 0.3830, Train Acc: 0.8984, Val Loss: 0.6678, Val Acc: 0.7188\n","Epoch 14/50, Train Loss: 0.3911, Train Acc: 0.8906, Val Loss: 0.5404, Val Acc: 0.7500\n","Epoch 15/50, Train Loss: 0.3077, Train Acc: 0.9141, Val Loss: 0.6277, Val Acc: 0.7500\n","Epoch 16/50, Train Loss: 0.3592, Train Acc: 0.8984, Val Loss: 0.8536, Val Acc: 0.6562\n","Epoch 17/50, Train Loss: 0.3529, Train Acc: 0.8828, Val Loss: 0.5178, Val Acc: 0.8125\n","Epoch 18/50, Train Loss: 0.3416, Train Acc: 0.8906, Val Loss: 0.7166, Val Acc: 0.8125\n","Epoch 19/50, Train Loss: 0.2535, Train Acc: 0.9453, Val Loss: 0.7633, Val Acc: 0.7500\n","Epoch 20/50, Train Loss: 0.2770, Train Acc: 0.9141, Val Loss: 0.6097, Val Acc: 0.7812\n","Epoch 21/50, Train Loss: 0.2452, Train Acc: 0.9531, Val Loss: 0.6756, Val Acc: 0.8438\n","Epoch 22/50, Train Loss: 0.2514, Train Acc: 0.9453, Val Loss: 0.7280, Val Acc: 0.7500\n","Epoch 23/50, Train Loss: 0.2467, Train Acc: 0.9688, Val Loss: 0.8053, Val Acc: 0.7500\n","Epoch 24/50, Train Loss: 0.2468, Train Acc: 0.9453, Val Loss: 0.6733, Val Acc: 0.7812\n","Epoch 25/50, Train Loss: 0.2214, Train Acc: 0.9688, Val Loss: 0.8026, Val Acc: 0.6875\n","Epoch 26/50, Train Loss: 0.2122, Train Acc: 0.9609, Val Loss: 0.6683, Val Acc: 0.7500\n","Epoch 27/50, Train Loss: 0.2441, Train Acc: 0.9453, Val Loss: 0.7086, Val Acc: 0.7812\n","Epoch 28/50, Train Loss: 0.2141, Train Acc: 0.9531, Val Loss: 0.6255, Val Acc: 0.7188\n","Epoch 29/50, Train Loss: 0.1751, Train Acc: 0.9688, Val Loss: 0.5874, Val Acc: 0.7812\n","Epoch 30/50, Train Loss: 0.2067, Train Acc: 0.9297, Val Loss: 0.6434, Val Acc: 0.7812\n","Epoch 31/50, Train Loss: 0.1752, Train Acc: 0.9531, Val Loss: 0.6635, Val Acc: 0.8125\n","Epoch 32/50, Train Loss: 0.1679, Train Acc: 0.9688, Val Loss: 0.7749, Val Acc: 0.7812\n","Epoch 33/50, Train Loss: 0.1486, Train Acc: 0.9688, Val Loss: 0.8447, Val Acc: 0.7812\n","Epoch 34/50, Train Loss: 0.2179, Train Acc: 0.9297, Val Loss: 0.7129, Val Acc: 0.8125\n","Epoch 35/50, Train Loss: 0.1617, Train Acc: 0.9844, Val Loss: 0.6961, Val Acc: 0.8125\n","Epoch 36/50, Train Loss: 0.1491, Train Acc: 0.9531, Val Loss: 0.7485, Val Acc: 0.7812\n","Epoch 37/50, Train Loss: 0.1303, Train Acc: 0.9844, Val Loss: 0.8006, Val Acc: 0.7812\n","Epoch 38/50, Train Loss: 0.2202, Train Acc: 0.9219, Val Loss: 0.7314, Val Acc: 0.8125\n","Epoch 39/50, Train Loss: 0.1618, Train Acc: 0.9688, Val Loss: 0.7304, Val Acc: 0.8125\n","Epoch 40/50, Train Loss: 0.1635, Train Acc: 0.9375, Val Loss: 0.7398, Val Acc: 0.7812\n","Epoch 41/50, Train Loss: 0.1976, Train Acc: 0.9375, Val Loss: 0.7042, Val Acc: 0.7500\n","Epoch 42/50, Train Loss: 0.1626, Train Acc: 0.9609, Val Loss: 0.6828, Val Acc: 0.7500\n","Epoch 43/50, Train Loss: 0.1726, Train Acc: 0.9531, Val Loss: 0.8794, Val Acc: 0.7188\n","Epoch 44/50, Train Loss: 0.1495, Train Acc: 0.9609, Val Loss: 0.6597, Val Acc: 0.8125\n","Epoch 45/50, Train Loss: 0.1220, Train Acc: 0.9688, Val Loss: 0.6457, Val Acc: 0.7188\n","Epoch 46/50, Train Loss: 0.1572, Train Acc: 0.9531, Val Loss: 0.7521, Val Acc: 0.7812\n","Epoch 47/50, Train Loss: 0.1692, Train Acc: 0.9688, Val Loss: 0.9149, Val Acc: 0.7500\n","Epoch 48/50, Train Loss: 0.1656, Train Acc: 0.9688, Val Loss: 0.7534, Val Acc: 0.8438\n","Epoch 49/50, Train Loss: 0.1311, Train Acc: 0.9766, Val Loss: 0.9090, Val Acc: 0.7812\n","Epoch 50/50, Train Loss: 0.1895, Train Acc: 0.9375, Val Loss: 0.8073, Val Acc: 0.7812\n","Test Accuracy: 0.6750\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z9-PebhcB27l"},"execution_count":null,"outputs":[]}]}